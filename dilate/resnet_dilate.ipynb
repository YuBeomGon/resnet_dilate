{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intermediate-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import cv2\n",
    "import os\n",
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from resnet_dilate import *\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collaborative-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict \n",
    "args = easydict.EasyDict({ \"batch_size\": 256, \n",
    "                          \"epochs\": 90, \n",
    "                          \"data\": 0, \n",
    "                          'arch':'resnet18',\n",
    "                          'lr':0.1,\n",
    "                         'momentum':0.9,\n",
    "                         'weight_decay':1e-4,\n",
    "                         'start_epoch':0,\n",
    "                         'gpu':0,\n",
    "                          'print_freq':2500,\n",
    "                          'workers' : 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65ad935-4736-4cb3-8149-bfe2f463385a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "subtle-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = resnet18(pretrained=False)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                            momentum=args.momentum,\n",
    "                            weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "linear-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "data_dir = '../ILSVRC/Data/CLS-LOC/'\n",
    "traindir = os.path.join(data_dir, 'train')\n",
    "valdir = os.path.join(data_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "described-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    traindir,\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "val_dataset = datasets.ImageFolder(valdir, transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "associate-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = None\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),\n",
    "    num_workers=args.workers, pin_memory=True, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "celtic-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cubic-dancing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (auen): AutoEncoder(\n",
       "    (conv1): Conv2d(3, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "    (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (gelu): GELU()\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (upsample1): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (upsample2): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (upsample3): ConvTranspose2d(8, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (bn4): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (loss): MSELoss()\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (gelu): GELU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (gelu): GELU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (gelu): GELU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (gelu): GELU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (gelu): GELU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (gelu): GELU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (gelu): GELU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (gelu): GELU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accurate-venezuela",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon/anaconda3/envs/pytorch_retina/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][   0/5005]\tTime  2.987 ( 2.987)\tData  2.407 ( 2.407)\tLoss 8.2495e+00 (8.2495e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.39 (  0.39)\n",
      "Epoch: [0][3000/5005]\tTime  0.332 ( 0.346)\tData  0.000 ( 0.001)\tLoss 4.4154e+00 (5.4483e+00)\tAcc@1  13.67 (  6.82)\tAcc@5  35.16 ( 18.05)\n",
      "Test: [   0/1565]\tTime  0.345 ( 0.345)\tLoss 3.2756e+00 (3.2756e+00)\tAcc@1  34.38 ( 34.38)\tAcc@5  53.12 ( 53.12)\n",
      " * Acc@1 20.695 Acc@5 44.294\n",
      "************train_loss None val_acc 20.694753646850586*************\n",
      "epoch time 1805.364753961563\n",
      "Epoch: [1][   0/5005]\tTime  1.889 ( 1.889)\tData  1.770 ( 1.770)\tLoss 3.7772e+00 (3.7772e+00)\tAcc@1  24.22 ( 24.22)\tAcc@5  47.66 ( 47.66)\n",
      "Epoch: [1][3000/5005]\tTime  0.357 ( 0.344)\tData  0.000 ( 0.001)\tLoss 3.4864e+00 (3.6296e+00)\tAcc@1  28.12 ( 26.45)\tAcc@5  52.34 ( 50.28)\n",
      "Test: [   0/1565]\tTime  0.343 ( 0.343)\tLoss 1.4319e+00 (1.4319e+00)\tAcc@1  75.00 ( 75.00)\tAcc@5  84.38 ( 84.38)\n",
      " * Acc@1 31.777 Acc@5 57.925\n",
      "************train_loss None val_acc 31.777233123779297*************\n",
      "epoch time 1797.173376083374\n",
      "Epoch: [2][   0/5005]\tTime  1.926 ( 1.926)\tData  1.781 ( 1.781)\tLoss 3.1090e+00 (3.1090e+00)\tAcc@1  36.72 ( 36.72)\tAcc@5  58.20 ( 58.20)\n",
      "Epoch: [2][3000/5005]\tTime  0.309 ( 0.346)\tData  0.000 ( 0.001)\tLoss 3.0108e+00 (3.1181e+00)\tAcc@1  34.38 ( 34.50)\tAcc@5  60.94 ( 59.66)\n",
      "Test: [   0/1565]\tTime  0.346 ( 0.346)\tLoss 3.0528e+00 (3.0528e+00)\tAcc@1  37.50 ( 37.50)\tAcc@5  62.50 ( 62.50)\n",
      " * Acc@1 31.230 Acc@5 56.873\n",
      "************train_loss None val_acc 31.229900360107422*************\n",
      "epoch time 1801.573974609375\n",
      "Epoch: [3][   0/5005]\tTime  1.771 ( 1.771)\tData  1.630 ( 1.630)\tLoss 2.7374e+00 (2.7374e+00)\tAcc@1  41.41 ( 41.41)\tAcc@5  65.23 ( 65.23)\n",
      "Epoch: [3][3000/5005]\tTime  0.386 ( 0.347)\tData  0.000 ( 0.001)\tLoss 2.8359e+00 (2.8908e+00)\tAcc@1  37.89 ( 38.42)\tAcc@5  65.62 ( 63.75)\n",
      "Test: [   0/1565]\tTime  0.374 ( 0.374)\tLoss 1.8186e+00 (1.8186e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  84.38 ( 84.38)\n",
      " * Acc@1 38.307 Acc@5 64.823\n",
      "************train_loss None val_acc 38.30726623535156*************\n",
      "epoch time 1809.0061609745026\n",
      "Epoch: [4][   0/5005]\tTime  2.166 ( 2.166)\tData  2.047 ( 2.047)\tLoss 2.5392e+00 (2.5392e+00)\tAcc@1  44.53 ( 44.53)\tAcc@5  71.48 ( 71.48)\n",
      "Epoch: [4][3000/5005]\tTime  0.306 ( 0.345)\tData  0.000 ( 0.001)\tLoss 2.8209e+00 (2.7609e+00)\tAcc@1  38.28 ( 40.79)\tAcc@5  65.62 ( 66.01)\n",
      "Test: [   0/1565]\tTime  0.348 ( 0.348)\tLoss 1.8581e+00 (1.8581e+00)\tAcc@1  71.88 ( 71.88)\tAcc@5  81.25 ( 81.25)\n",
      " * Acc@1 39.951 Acc@5 66.447\n",
      "************train_loss None val_acc 39.95125961303711*************\n",
      "epoch time 1806.8099236488342\n",
      "Epoch: [5][   0/5005]\tTime  2.130 ( 2.130)\tData  1.983 ( 1.983)\tLoss 2.5729e+00 (2.5729e+00)\tAcc@1  43.75 ( 43.75)\tAcc@5  66.02 ( 66.02)\n",
      "Epoch: [5][3000/5005]\tTime  0.331 ( 0.346)\tData  0.000 ( 0.001)\tLoss 2.9628e+00 (2.6830e+00)\tAcc@1  37.11 ( 42.12)\tAcc@5  63.28 ( 67.25)\n",
      "Test: [   0/1565]\tTime  0.353 ( 0.353)\tLoss 1.1408e+00 (1.1408e+00)\tAcc@1  71.88 ( 71.88)\tAcc@5  90.62 ( 90.62)\n",
      " * Acc@1 43.862 Acc@5 70.534\n",
      "************train_loss None val_acc 43.86248779296875*************\n",
      "epoch time 1806.3578085899353\n",
      "Epoch: [6][   0/5005]\tTime  2.292 ( 2.292)\tData  2.139 ( 2.139)\tLoss 2.4030e+00 (2.4030e+00)\tAcc@1  48.44 ( 48.44)\tAcc@5  75.78 ( 75.78)\n",
      "Epoch: [6][3000/5005]\tTime  0.388 ( 0.364)\tData  0.000 ( 0.001)\tLoss 2.6588e+00 (2.6208e+00)\tAcc@1  42.97 ( 43.21)\tAcc@5  65.62 ( 68.33)\n",
      "Test: [   0/1565]\tTime  0.342 ( 0.342)\tLoss 1.5197e+00 (1.5197e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  84.38 ( 84.38)\n",
      " * Acc@1 41.745 Acc@5 68.476\n",
      "************train_loss None val_acc 41.74507141113281*************\n",
      "epoch time 1904.6252615451813\n",
      "Epoch: [7][   0/5005]\tTime  2.353 ( 2.353)\tData  2.202 ( 2.202)\tLoss 2.4862e+00 (2.4862e+00)\tAcc@1  46.48 ( 46.48)\tAcc@5  71.48 ( 71.48)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0fe882d453f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_auen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch/imagenet/main.py\u001b[0m in \u001b[0;36mtrain_auen\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, args)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mau_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0;31m# measure accuracy and record loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0macc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc1 = 0\n",
    "acc1 = 0\n",
    "train_loss = []\n",
    "val_acc = []\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    start_time = time.time()\n",
    "    adjust_learning_rate(optimizer, epoch, args)\n",
    "\n",
    "    # train for one epoch\n",
    "    epoch_loss = train_auen(train_loader, model, criterion, optimizer, epoch, args)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    acc1 = validate_auen(val_loader, model, criterion, args)  \n",
    "    \n",
    "    train_loss.append(epoch_loss)\n",
    "    val_acc.append(acc1)\n",
    "    print('************train_loss {} val_acc {}*************'.format(epoch_loss, acc1))\n",
    "    \n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "#     if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n",
    "#             and args.rank % ngpus_per_node == 0):\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': args.arch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)\n",
    "    \n",
    "    print('epoch time', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-sally",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-rebel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-dominican",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.tensor([[ 1.6376, -0.2284,  0.0860],\n",
    "        [ 0.1055,  1.9402, -0.0046],\n",
    "        [-0.0257, -0.2146,  1.5572]], device='cuda:0', requires_grad=True).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(weight,  torch.inverse(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.inverse(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.inverse(model.LT.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-string",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/Dataset/scl/patch_images/2021.01.15/LBC521-20210112(1)/LBC521-20210112(1)_1006.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread(path)\n",
    "# img = np.resize(img, (256,256,3))\n",
    "# img = torch.tensor(img)/255.\n",
    "# img.shape\n",
    "img = torch.randn(2,256,256,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = nn.Parameter(torch.eye(3, dtype=torch.float), requires_grad=True)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "img[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.matmul(img, weight)\n",
    "# res[0]\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-horse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
